---
title: 博士四年总结
date: 2025-11-09 00:00:00 +0800
class: "article"  # Override the default card style
width: 3
---

### 博士四年总结

我永远也无法忘记这一段旅途，攻读博士学位的四年学习生涯，是我迄今为止最有趣也最具挑战的一段经历。

<p align="right">— 秦铁鑫  2025年11月8日</p>

欲买桂花同载酒，终不似，少年游。在鲜衣怒马的朝华之龄，你会选择投入怎样的人生？我的选择是读博。在十月二号正式被授予博士学位为止，这段旅途终于走向了终点。思虑良久，终于还是决定要将其整理出来，一为总结与自省，二为明未来道路。

- 缘起


2021年八月，彼时的我尚处在硕士三年十一投一中的阴霾中，未来的道路怎么走还未明晰，就这样在一半晦暗一半忐忑中开启了我的博士生涯。我的导师[李皓亮](https://hlli1991.github.io/)老师是最早研究域泛化(domain generalization)的两个人之一，加之我的背景也多与分布偏移(distribution shift)相关，所以不出意外的我也是跟着这个方向做。李老师给我指定的方向是研究动态环境下的机器学习系统。我们知道，现在的人工智能算法所倚赖的一个基本假设是独立同分布(i.i.d)假设，即训练数据和测试数据是从同一个分布中独立采样得到的。这一假设的局限性在于当测试数据的分布与训练数据出现偏差时，基于训练数据训练得到的模型往往在测试数据上表现很差。这种被称为分布偏移的现象在许多实际场景中屡见不鲜，比如在自动驾驶系统中，如果我们仅采集到白天道路上的数据训练了一个车辆检测模型，而在实际部署这个系统时，我们需要这个检测模型在即便是夜晚的状况下也提供较高的检测精度。我的导师的代表作就是在试图解决这个域泛化问题，可是我们想想，白天到夜晚的这种变化并不是瞬间发生的，它是有一个过渡存在，那么很自然的引出一个问题，我们的模型能否学习并利用这种过渡的模式(pattern)来帮助其取得更好的泛化效果呢？为此，我们形式化地定义了这个问题为：训练数据不再是同一域，而是由一个域序列构成(早上、中午、下午、傍晚)，同样的，测试域也是多个域构成的序列(晚上、深夜、凌晨...)。根据贝叶斯定理 $P(X,Y)=P(X)P(Y\mid X)$ 可知，在动态环境中，数据的分布偏移可分解为协方差偏移(covariant shift，对应 $P(X)$ 上的变化)与概念偏移(concept shift，对应 $P(Y\mid X)$ 上的变化)。为了让模型能同时学到这两种偏移的动态变化，我们构建了一个基于序列变分自编码器结构的概率建模框架，在隐空间中通过变分推理的形式推导出每个时刻协方差与概念偏移的变化。因为这是我的博士第一个工作，成稿时写作惨不忍睹，李老师大改了好几遍，万幸，这篇工作一次就中了，发表在了ICML上[^1]。不过时值疫情，未能去北美开会。


在完成这个工作后，我们就在思考有没有办法将这个方法作进一步增强呢。我们知道，变分推理是利用KL散度来进行先验分布与后验分布对齐的，由KL散度的计算公式为 $D_{KL}(p(x),q(x))=\int p(x)\log \frac{p(x)}{q(x)}$ 可知这个计算过程中会涉及除0的问题(即 $q(x)=0$ )，当此情况发生时，模型的训练会变得极度不稳定。虽然我们在前面的工作中设计了一个时序上的约束来帮助稳定这个过程，但并不能完全规避掉这个问题，所以我们第二个工作想解决的就是KL散度引发的不稳定问题。我们想出的解决方案是将KL距离替换为不涉及除法操作的MMD ($D_{\mathrm{MMD}}(q,p) := \left\Vert E_{\mathbf{z}^q \sim q} [\phi(\mathbf{z}^q)] - E_{\mathbf{z}^p \sim p} [\phi(\mathbf{z}^p)] \right\Vert^2_{\mathcal{H}_k}$) 这样自然就消除了除0的隐患，并且在计算上可以利用核技巧(kernel trick)来避免显式对数据分布的估计。此外，我们在文章中理论上验证了这种距离替换还隐式地引入了三个互信息最大化项(对应三个隐变量)可以帮助我们的模型在隐空间学到更好的特征表示。这篇工作的完成花费了半年时间，在这个过程自己补了非常多测度论的知识，将[Bernhard Schölkopf](https://scholar.google.com/citations?user=DZ-fHPgAAAAJ&hl=en)从two-sample test到MMD的提出一系列论文看了个遍。这篇文章也在经过两轮修改后顺利地被TPAMI接受[^2]。


在做这两个工作的过程中，我对机器学习理论以及数学原理产生了浓厚的兴趣，变分、测度，一个个概念像是提供给我一个个不同看待世界的视角，令我目眩神迷。此时的我才刚经过一年，心仪的顶会有了，顶刊也在路上，正所谓“春风得意马蹄疾，一日看尽长安花”，即便最终以此两篇收场，也足以满足大多数学校的毕业要求了。此时的我，来不及等待TPAMI落地为安便按耐不住地奔向了理论的海洋，我要以数学理论为基石，以想象力构筑属于自己的世界。此时，恰逢我们组刚开始与牛津数学系的老师开展合作，[Terry Lyons](https://www.maths.ox.ac.uk/people/terry.lyons)教授是领域泰斗，他所提出的rough path theory是处理动态系统中非平稳信号的一个强有力的工具。其中的受控微分方程(controlled differential equation)将常规微积分以时间这种均质信号驱动拓展为一个path驱动，这样就可以处理高度震荡且非连续的信号(比如Wiener Process)。我探究的是如何基于此理论构建一个动态环境下的机器学习系统。这里，我们仍然要回归到独立同分布假设上，即便我们前面所做的工作将独立同分布假设拓展到时序场景上，即每个时刻的数据分布是独立同分布的，但相邻时刻的数据分布却具备某种连续性，那么我们能否把每个时刻的独立同分布假设也去掉呢？如果还是以自动驾驶系统为例，这里我们在路面车辆检测完成后搞了个行车路线规划模型，路线规划显然是受周围车辆临近关系影响的，而周围的车辆分布并不满足独立同分布情况，他们存在某种空间上的结构。我们将这种空间上的临近关系用图(graph)来表示，因为周围的车辆是不断变化的，所以这里的图也是动态变化的。我们提出了一个图神经受控微分方程: $\mathbf{Z}_t = \mathbf{Z}_{t_0} + \int^t_{t_0} f_\theta(\mathbf{Z}_s, \mathbf{A}_s)\mathrm{d}\hat{\mathbf{A}}_s$, 这里 $\mathbf{A}_s$ 表示时间为时的图的邻接矩阵，可以看到图结构的变化直接驱动了节点状态的变化。这篇工作最早投往COLT，可惜因不够学习理论而被拒，随后投往TPAMI上，也是在历经三年后于近期才被接收。这篇是我个人最喜欢的工作但也是经历最为坎坷的，除了被COLT拒，在TPAMI上也因为找不到合适的审稿人以及更换副主编两年多才返回审稿意见，前前后后我与小伙伴[Ben](https://www.maths.ox.ac.uk/people/benjamin.walker1)修改了多次才终于被接收[^3]。Terry对这篇工作也是多加赞赏，在我于牛津访问的第一天，Terry说他将文章分发给了他的学生们看，此后其学生[Torben](https://scholar.google.com/citations?user=rhrW8dAAAAAJ&hl=en)也是与我合作开发了我们工作后续的图置换同变版本并被今年的NeurIPS接收[^4]。

时间来到2023年五月，也就是我的博二下半年，在等待前个工作审稿的时间里我并没停下脚步。这时使用物理神经网络来建模动态过程属于一个热点方向，不管出身力学、应数还是仿真领域的都往这上面做。我也非常看好这个方向，试想原先对于流体动态的仿真不管是有限元还是有限差分都需要先建立网格，但是网格建完后其运算天然满足并行运算要求，也就是我们可以用GPU来做这个计算，同时，高精度仿真所需的高昂的计算复杂度也可以通过神经网络的非线性拟合能力去近似求解。在这两点的促进下，很多顶尖的高校如MIT、Stanford还有企业Nvidia、Meta入局。这些工作主要验证的是这件事的可行性到底如何，采用的措施是设计尽可能强大的物理神经网络来提升对系统动态的计算精度。但这些工作仅聚焦在了对单个动态系统的拟合上，而对所设计的神经网络求解器的泛化能力探究很少。这点与传统的数值求解器完全不同，比如我们使用Navier Stokes方程来做流体仿真，我们可以通过简单地修改粘度、雷诺数(Reynolds number)等来让我们的方程适合不同海拔高度气体流动的模拟。我们也想让我们的物理神经网络具备这种快速适用不同使用场景的能力，继续拿行车路线规划模型为例，就是我们使用同一套驾驶系统，但是车子变了、传感器不一致，我们可以修改少量的参数而不需要重新训练一个模型。我们基于的是傅立叶变换(Fourier transform)搭建的神经网络，傅立叶变换的好处在于，对于时域上的一个连续动态信号，我们在频域以多个三角函数线性相加形式来表示它，这样我们在频域上对某个三角函数的系数进行一次的调整，就可以一直影响傅立叶逆变换后的整个时域上的动态信号了。我们先设计了一个分解器，将这些三角函数划分为多个动态环境共享的以及每个动态环境专有的，这样我们在将我们的神经网络部署到某个实际的动态场景上时，就可以仅更新动态环境专有对应的三角函数们。这个工作从有idea到第一次投稿也就一个多月的时间，是我做的最快的一个工作。当然，后续并不顺利，也是几经修改后方被接收[^5]。

在完成上面的这个工作后，我开始试图去解决一个公开问题(open question)。在域泛化领域，这个问题只有一个——神经网络的泛化边界问题。当我们训练了一个神经网络模型，我们想知道这个模型可以在哪些未见过的场景上工作呢？有没有理论可以引导我们设计更具泛化性的模型呢？就拿目前最火的大模型来说，它的能力是依赖于训练时使用了几乎地球上所有的数据，所以它的回答即便仅是对这些已有数据进行内插(interpolation)，也会让我们感慨其“智能”。但是当数据达到极限，我们怎么去进一步提升其能力呢？这就需要它能进行外推(extrapolation)，也就是泛化, 这个问题非常重要，但一直都没得到很好的解决。这里最大的难点在于我们如何定义这些未见过的数据，它们的分布该是什么样，与原先的训练数据存在什么样的关系。早期我的探索思路是沿着已有的学习理论出发，逐步向域泛化拓展。现有的学习理论都在解决有限样本的泛化问题，具体而言，训练数据是对某个分布进行有限次采样得到的，这些数据必然不能代表这个分布所有的信息，如果拿这些训练数据来训练一个模型，它在对这个分布进行无限次采样得到的测试样本上的测试表现是什么样的。可以看到，这个问题仍然局限于同一分布上，无论是Rademacher Complexity、Over-parameterization theory 还是Neural tangent kernel莫不如此。我很喜欢Neural tangent kernel，它说一个无限宽的神经网络基于梯度流和平方损失训练时就等同于核回归。这里我做了几个尝试，其一是将梯度流看作一个动态系统来处理；其二是将这些未见过的域以某种图结构组合起来；当然还有好些开脑洞的想法，欢迎对这个方向感兴趣的小伙伴一起讨论。这件事在与[Jared Tanner](https://www.maths.ox.ac.uk/people/jared.tanner)教授讨论后发现现有的方法都不太好做，挣扎了一个月后遂放弃。

在我读博的第三年，除了对公开问题的挑战外，也开始对交叉领域进行挑战。适逢严老师那边有些针对肺癌的药物开发数据，具体说来，肺癌中最常见的类型为非小细胞肺癌，这种癌症是由于表皮生长因子受体发生突变引发的，同时突变还有可能导致细胞出现抗药性进而原有的抗癌药物失效。严老师希望我能够建立一种度量方式，能够有效地辨别细胞突变是良性的还是恶性的。一般而言，在细胞尤其是蛋白质发生突变后，其结构就无法保持稳定状态，会经历一个从非平稳态到平稳态的动态过程。在这个过程中，其整体的自由能逐步缩小到最小值，所以在分子动力学领域最常用的一种度量方式是计算前后过程中自由能的差异。这个动态过程同样适用于药物开发，药物与我们体内的蛋白质要在指定的靶点结合后才能生效，其自由能同样是逐渐减小并最终到稳定态。但是明显的，基于自由能的度量只能提供非常粗糙的信息，并不具备对结合位点、化学键断裂以及结合等这种细粒度信息提取的能力。我们提出的解决方案是将动态过程所有原子层级的运动轨迹作为输出，一般来说，一个蛋白质所包含的原子数在万、十万这个数量级，我们首先要做的是将这个规模缩减下来，为了不损失精度，我们使用了一种可学习的图切算法根据分子图的连接关系将其这些原子划分为一个个簇，在引入一个线性函数对每个簇的原子们进行聚合，这个过程也被叫做粗粒化(coarse graining)。完成这个操作后，我们得到了数量大大缩减后的原子轨迹。对于这些非线性动态且可能包含原子间的动态交互，我们使用路径签名(path signature)来将其提取为在希尔伯特空间中的一个表示。path signature同样是Terry提出的，这是将路径编码为迭代积分序列的一种非交换式的变换，它的优点在于可以将对于高度震荡的受控微分方程的解表示为对路径签名的线性处理。同时，相较于傅立叶变换，它可以捕获不同通道之间也就是原子间的交互作用。最后，我们仅需要使用一个简单的分类器就可以解决我们的任务。这样设计的一个好处是，path signature是置换同变的，又因为粗粒化阶段的聚合函数是线性的，我们可以反向检索出是哪些原子以及它们的交互导致了最终的结果。坦率而言，这个解决方案并不完美，一开始我们是想给出一个纯理论计算的框架，在我于牛津访学期间也是这样努力的，可这件事时至今日也没有完成。后来还是在我导师的建议下，利用神经网络的拟合能力先给出一个近似解来[^6]。此外，这篇工作的rebuttal也值得说道说道。一开始审稿意见刚出时，三个审稿人给了653，一个弱接收，一个给了弱拒，还有一个给了强拒。但是给弱接收的审稿人给出了非常专业的意见，另外两个人被带回正轨看到了我们这篇工作的价值，最后经过一个月的rebuttal期间解决了二十多个问题也是分数升到了866，一个强接受，两个弱接收。这篇工作是我花时间最多的一个工作，整个博三一年没有产出，在博四上学期才完成，历时一年多。如果之前的工作所浏览的论文数是40～50篇左右，这篇工作做完，我的zotero收藏论文的数量增加了800多篇。这个工作里最难的点在于问题选择，选择研究的问题会直接决定一篇工作的立意高低，而我们选择了一个很新颖的问题，这也是我们ICLR审稿中第一位审稿人所高度认可的部分。好玩的是，在这个过程中我也学到了非常多的知识，物理、化学、生物学等都涉及到了，也是狠狠恶补了一番。另外一个感受是，这个工作其实涉及到了三个公开问题：粗粒化、非线性动态以及大型的分子数据，给我的启发在于也许我可以通过找寻一个合适的公开问题来达成自己想要解决一个公开问题的目标，而我貌似已经找到了……

最后，我想谈谈个人感悟，在AI的浪潮下，不管是领域内的研究人员还是领域外的大众其实都被裹挟着前进。作为一名从业者，是顺势而为追逐热点还是逆流而上溯本求源是我一直在思考的问题。从目前的工作来看，我更喜欢逆流而上，理论会带给我一种明晰感，一种不单单是知道模型效果这么好，还知道这个模型为何效果这么好的畅然。但是我选择的这些研究问题并不old school，比如我们可以将前面提到的自动驾驶的例子换成股票、气流等等，实际上确实有猎头来找我去搞量化，而AI做量化投资正是现今金融领域的一个发展方向。此外，我的师弟[丁博](https://beauding.github.io/)将我们第一个工作用在了语音和视频上，好友[Abde](https://scholars.cityu.edu.hk/en/persons/abmeldaly2/)也与我时常讨论如何基于第三个工作来破译处在心流状态下人脑神经元信号传输的模式，我们做泛化理论的工作(没做出来)可以给现在大模型的训练指明方向等等。这些问题都是尚未解决且历久弥新。同时，积极地与合作者们讨论也会帮助我更新自身的知识库，我的合作者们[Kecheng](https://tonyckc.github.io/), [Jie Liu](https://scholar.google.com/citations?user=k05bkIEAAAAJ&hl=en), [Hui Liu](https://scholar.google.com/citations?user=BsSgtz4AAAAJ&hl=en)等等，与他们讨论极大地拓展了我对于新技术的理解。当然，一个新技术的产生也会引出它专有的一些问题，目前发现大模型中比较有意思的问题是reasoning。根据本体论有三个层面即客观存在、观察现象以及认知模型。客观存在指客观存在的物质世界，观察现象指人类对物质世界的观测，认知模型指我们的心智认知模型用来解释世界规律变化。科学就是这三个层面的相互影响。我们先针对一个自然现象设计观测手段来收集一定的数据，然后建立一个理论模型来拟合、解释这个数据，并获得我们对这个自然现象新的理解(即知识)。对于大模型，如果客观存在指人类的智能，大模型所使用的训练数据就是观测的数据，而大模型则对应于我们基于观测数据构建的模型，但是我们目前仅仅建立了一个对与数据高度拟合的一个模型，这个模型如何工作的，其学习模式是什么，我们不得而知，而完成这一步或许可以帮助我们开发下一代更强人工智能技术以及提升对自身大脑的利用效率。当然，人工智能的发展远不止这一个挑战，现今我们更要思考怎么去定义“智能”，这个智能需要像人类思考模式那样才能称之为智能吗？我有时在想，或许我们该拿掉“人工智能”里的“人工”二字，现今的人工智能技术，尤其是深度学习技术，虽然基于神经网络，但神经网络的基本单元与神经元的运作机理完全不同，在这种情况下，我们并不需要人工智能算法以模仿人类智能的形式发展，其基本单元的不同决定了它与人类智能存在本质上的不同，如果拿大模型举例，现今主流的大模型在知识面上完全吊打人类个体，这是它的优势，我们人脑塞不下这么多的知识。也许我们该放下人本位的偏差，看见人工智能自身的独特性来帮助其更好地发展。

最后的最后，科研是一方面，生活也是一方面，找寻到两者之间的平衡点，才能让自己一直走下去。博士四年里我将大部分时间投入了科研，做了上述这些自己很感兴趣的工作，但是人不能只有工作。前段时间，我回归了母校，见到了我大学时的辅导员琳姐还有班主任重秋哥，七年过去，他们依旧那般亲切又关照我；在校园闲逛时，朦胧间，我仿佛看到了大学时的自己，青春、恣意、活力满满。希望未来的自己可以将自己找回来，然后好好吃饭，好好睡觉，好好地玩。



[^1]: Tiexin Qin, Shiqi Wang, Haoliang Li. “Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder”. ICML, 2022.
[^2]: Tiexin Qin, Shiqi Wang, Haoliang Li. Evolving Domain Generalization via Latent Structure-Aware Sequential Autoencoder. TPAMI, 2023.
[^3]: Tiexin Qin, Benjamin Walker, Terry Lyons, Hong Yan, Haoliang Li. “Learning Dynamic Graph Embeddings with Neural Controlled Differential Equations”. TPAMI, 2025.
[^4]: Torben Berndt, Benjamin Walker, Tiexin Qin, Jan Stühmer, Andrey Kormilitzin. “Permutation Equivariant Neural Controlled Differential Equations for Dynamic Graph Representation Learning”. NeurIPS, 2025.
[^5]: Tiexin Qin, Hong Yan, Haoliang Li. “Generalize to New Dynamical Systems via Frequency Domain Adaptation”. TPAMI, 2025.
[^6]: Tiexin Qin, Mengxu Zhu, Chunyang Li, Terry Lyons, Hong Yan, Haoliang Li. “Deep Signature: Characterization of Large-Scale Molecular Dynamics”. ICLR, 2025.